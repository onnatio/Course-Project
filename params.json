{"name":"Course Project for Practical Machine Learning","tagline":"","body":"\r\n\r\n\r\n**Course Project for Practical Machine Learning**\r\n\r\nRmd-FILE:\r\n\r\n---\r\ntitle: \"Course Project for Practical Machine Learning\"\r\noutput: html_document\r\n---\r\n\r\n\r\nFirst, I load the caret package and the data file named \"pml-training\":\r\n\r\n```{r}\r\nlibrary(caret)\r\ntraining<-read.csv(\"C:/Users/Antonio/Documents/R/pml-training.csv\",header=TRUE)\r\n```\r\n\r\nI split the data set into training and testing subsets, named \"train\" and \"test\", respectively.\r\n\r\n```{r}\r\nset.seed(8090)\r\ninTrain<-createDataPartition(y=training$classe,p=0.75,list=FALSE)\r\ntrain<-training[inTrain,]\r\ntest<-training[-inTrain,]\r\ndim(training)\r\ndim(train)\r\ndim(test)\r\n```\r\n\r\nIn order to predict the variable \"Classe\", I use the Random Forest algorithm in the package \"caret\".I do not include in the algorithm variables with too many NA's or variables which do not give valuable information (like usernames, etc.).\r\n\r\n```{r}\r\nmod_rf = train(classe~raw_timestamp_part_1+raw_timestamp_part_2\r\n              +new_window+num_window+roll_belt+pitch_belt\r\n              +yaw_belt+total_accel_belt+gyros_belt_x+gyros_belt_y\r\n              +gyros_belt_z+accel_belt_x+accel_belt_y+accel_belt_z\r\n              +magnet_belt_x+magnet_belt_y+magnet_belt_z+roll_arm\r\n              +pitch_arm+yaw_arm+total_accel_arm+gyros_arm_x\r\n              +gyros_arm_y+gyros_arm_z+accel_arm_x+accel_arm_y+accel_arm_z\r\n              +magnet_arm_x+magnet_arm_y+magnet_arm_z+roll_dumbbell+pitch_dumbbell\r\n              +yaw_dumbbell, method = \"rf\", data = train) \r\n```\r\n\r\nThe results are shown here; we see that the estimate of the error rate is very low (0,1%); I also show the importance of the variables:\r\n\r\n```{r}\r\nprint(mod_rf$finalModel)\r\nvarImport<-varImp(mod_rf)\r\nvarImport\r\n```\r\n\r\nNext, I use the test subset to validate the model. I build the Confusion Matrix and we can see that the accuracy is very high, 0,998.\r\n\r\n```{r}\r\npreds<-predict(mod_rf,newdata=test)\r\nconfusionMatrix(preds,test$classe)\r\n```\r\n\r\nFinally, I use the model to predict the values of the variable \"Classe\" of a new set of data named \"pml-testing\". The 20 predictions are shown below:\r\n\r\n```{r}\r\ntesting<-read.csv(\"C:/Users/Antonio/Documents/R/pml-testing.csv\",header=TRUE)\r\ntesting_preds<-predict(mod_rf,newdata=testing)\r\ntesting_preds\r\n```\r\n\r\n\r\nCOMPILED HTML FILE:\r\n\r\nFirst, I load the caret package and the data file named “pml-training”:\r\n\r\nlibrary(caret)\r\n\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n\r\ntraining<-read.csv(\"C:/Users/Antonio/Documents/R/pml-training.csv\",header=TRUE)\r\n\r\nI split the data set into training and testing subsets, named “train” and “test”, respectively.\r\n\r\nset.seed(8090)\r\ninTrain<-createDataPartition(y=training$classe,p=0.75,list=FALSE)\r\ntrain<-training[inTrain,]\r\ntest<-training[-inTrain,]\r\ndim(training)\r\n\r\n## [1] 19622   160\r\n\r\ndim(train)\r\n\r\n## [1] 14718   160\r\n\r\ndim(test)\r\n\r\n## [1] 4904  160\r\n\r\nIn order to predict the variable “Classe”, I use the Random Forest algorithm in the package “caret”.I do not include in the algorithm variables with too many NA’s or variables which do not give valuable information (like usernames, etc.).\r\n\r\nmod_rf = train(classe~raw_timestamp_part_1+raw_timestamp_part_2\r\n              +new_window+num_window+roll_belt+pitch_belt\r\n              +yaw_belt+total_accel_belt+gyros_belt_x+gyros_belt_y\r\n              +gyros_belt_z+accel_belt_x+accel_belt_y+accel_belt_z\r\n              +magnet_belt_x+magnet_belt_y+magnet_belt_z+roll_arm\r\n              +pitch_arm+yaw_arm+total_accel_arm+gyros_arm_x\r\n              +gyros_arm_y+gyros_arm_z+accel_arm_x+accel_arm_y+accel_arm_z\r\n              +magnet_arm_x+magnet_arm_y+magnet_arm_z+roll_dumbbell+pitch_dumbbell\r\n              +yaw_dumbbell, method = \"rf\", data = train) \r\n\r\n## Loading required package: randomForest\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n\r\nThe results are shown here; we see that the estimate of the error rate is very low (0,1%); I also show the importance of the variables:\r\n\r\nprint(mod_rf$finalModel)\r\n\r\n## \r\n## Call:\r\n##  randomForest(x = x, y = y, mtry = param$mtry) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 17\r\n## \r\n##         OOB estimate of  error rate: 0.1%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 4183    0    0    2    0 0.0004778973\r\n## B    0 2847    1    0    0 0.0003511236\r\n## C    0    2 2564    1    0 0.0011686794\r\n## D    0    0    5 2406    1 0.0024875622\r\n## E    0    0    0    2 2704 0.0007390983\r\n\r\nvarImport<-varImp(mod_rf)\r\nvarImport\r\n\r\n## rf variable importance\r\n## \r\n##   only 20 most important variables shown (out of 33)\r\n## \r\n##                      Overall\r\n## raw_timestamp_part_1 100.000\r\n## num_window            55.112\r\n## roll_belt             39.468\r\n## yaw_belt              26.755\r\n## pitch_belt            19.403\r\n## roll_dumbbell         16.172\r\n## magnet_arm_x           9.242\r\n## yaw_dumbbell           8.619\r\n## magnet_belt_z          8.163\r\n## roll_arm               8.109\r\n## accel_belt_z           7.991\r\n## accel_arm_x            6.972\r\n## magnet_belt_y          6.682\r\n## yaw_arm                4.237\r\n## accel_arm_z            4.132\r\n## pitch_arm              4.097\r\n## magnet_belt_x          3.690\r\n## pitch_dumbbell         3.203\r\n## accel_arm_y            2.992\r\n## total_accel_belt       2.656\r\n\r\nNext, I use the test subset to validate the model. I build the Confusion Matrix and we can see that the accuracy is very high, 0,998.\r\n\r\npreds<-predict(mod_rf,newdata=test)\r\nconfusionMatrix(preds,test$classe)\r\n\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1394    1    0    0    0\r\n##          B    0  948    2    0    0\r\n##          C    0    0  853    2    0\r\n##          D    1    0    0  802    1\r\n##          E    0    0    0    0  900\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9986          \r\n##                  95% CI : (0.9971, 0.9994)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9982          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9993   0.9989   0.9977   0.9975   0.9989\r\n## Specificity            0.9997   0.9995   0.9995   0.9995   1.0000\r\n## Pos Pred Value         0.9993   0.9979   0.9977   0.9975   1.0000\r\n## Neg Pred Value         0.9997   0.9997   0.9995   0.9995   0.9998\r\n## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Detection Rate         0.2843   0.1933   0.1739   0.1635   0.1835\r\n## Detection Prevalence   0.2845   0.1937   0.1743   0.1639   0.1835\r\n## Balanced Accuracy      0.9995   0.9992   0.9986   0.9985   0.9994\r\n\r\nFinally, I use the model to predict the values of the variable “Classe” of a new set of data named “pml-testing”. The 20 predictions are shown below:\r\n\r\ntesting<-read.csv(\"C:/Users/Antonio/Documents/R/pml-testing.csv\",header=TRUE)\r\ntesting_preds<-predict(mod_rf,newdata=testing)\r\ntesting_preds\r\n\r\n##  [1] B A B A A E D B A A B C B A E E A B B B\r\n## Levels: A B C D E\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}